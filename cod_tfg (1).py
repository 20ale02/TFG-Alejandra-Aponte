# -*- coding: utf-8 -*-
"""Cod_TFG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11fkYGiPVnqLPbSZ2NBh_dzZXeXNLjgEI

CÓDIGO TFG


> María Alejandra Aponte Lerín

## INTRODUCCIÓN
En este documento se realizará el código para desarrollar la investigación sobre los riesgos quirúrgicos desde un enfoque estadístico.
Se utilizará una base de datos sobre riesgos neurológicos después de operaciones cardiovasculares, donde la variable respuesta se considerará 'neuro', que indica si el paciente ha padecido o no una enfermedad neurológica.

Se empezará realizando el EDA, para posteriormente relizar lo pertinentes preprocesos y los modelos que se consideren.

# EDA

En este apartado obtendremos una visión general de nuestros datos que nos permita examinar su posible preprocesamiento y posterior modelización.

## CARGAMOS LIBRERIAS
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install keras-tuner
!pip install lime
!pip install shap

# Importar librerías
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, roc_auc_score
import itertools # Importante
from sklearn.tree import plot_tree
from sklearn.ensemble import RandomForestClassifier
from shap import Explainer, Explanation
from shap import waterfall_plot
import shap
import lime
import lime.lime_tabular
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from google.colab import drive
#drive.mount('/content/drive')
import os
#os.chdir("/content/drive/MyDrive/TFG/cod TSVM/Twin-SVM-master")
from TVSVM import TwinSVMClassifier
from sklearn import preprocessing
from sklearn.base import BaseEstimator, ClassifierMixin
import KernelFunction as kf
import TwinPlane1
import TwinPlane2
#from sklearn.datasets import load_boston
from sklearn.datasets import fetch_california_housing
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
import keras_tuner as kt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import random as python_random
from sklearn import metrics
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from keras.callbacks import EarlyStopping
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import r2_score
from sklearn.impute import KNNImputer
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler
from sklearn.svm import SVR
from sklearn.model_selection import TimeSeriesSplit
import seaborn as sns
from sklearn.svm import SVC
from sklearn.svm import NuSVC
from sklearn.tree import DecisionTreeRegressor

"""## CARGAMOS DATASET"""

# Cargar el dataset
#datosneu = '/content/drive/MyDrive/TFG/TFG Alejandra/datos/Neurological-complications-after-cardiac-surgery/Database.xlsx'#
datosneu = 'Database.xlsx'
dfneu= pd.read_excel(datosneu)

# Explorar el dataframe
print(dfneu.shape) # cuántas instancias y columnas tiene la muestra
print(dfneu.columns) # nombre de las columnas
print(dfneu.dtypes) # tipos de las columnas
print(dfneu.describe()) # estadísticos

"""Tenemos 206 datos, con un total de 74 columnas.
Como se puede observar, se ha obtenido la media, varianza, mínimos, máximos y cuartiles de las variables.

## CONVIERTO LAS VARIABLES NECESARIAS AL TIPO DE DATO CORRECTO
"""

# Suponiendo que 'idstata' es el nombre de la columna que contiene los identificadores
dfneu['idstata'] = dfneu['idstata'].astype('object')

# Verificar que se haya convertido correctamente
print(dfneu.dtypes)

"""Convertimos surgtype"""

# Encuentra las columnas que contienen el carácter 'D'
columns_with_d = [col for col in dfneu.columns if 'D' in dfneu[col].astype(str).values]

# Imprime las columnas encontradas
print(f"Columnas que contienen 'D': {columns_with_d}")

# Supongamos que la columna que deseas ver se llama "nombre_columna"
# Accede a la columna completa
columna_completa = dfneu['surgtype']

# Imprime los primeros 5 valores de la columna (puedes ajustar la cantidad)
print(columna_completa.head())

# Si deseas ver toda la columna sin truncar, puedes imprimirla completa
print(columna_completa)

# Verifica el tipo de datos actual de la columna
print(f"Tipo de datos actual de mi_columna: {dfneu['surgtype'].dtype}")

# Define el diccionario de mapeo
mapeo = {'A': 1, 'B': 2, 'C': 3, 'D': 4}

# Aplica el mapeo a la columna 'letras'
dfneu['surgtype'] = dfneu['surgtype'].map(mapeo)


# Verifica el nuevo tipo de datos de la columna
print(f"Nuevo tipo de datos de mi_columna: {dfneu['surgtype'].dtype}")

# Accede a la columna completa
columna_completa = dfneu['surgtype']

# Imprime los primeros 5 valores de la columna (puedes ajustar la cantidad)
print(columna_completa.head())

# Si deseas ver toda la columna sin truncar, puedes imprimirla completa
print(columna_completa)

"""## CORRELACIONES

Vamos a realizar un breve análisis de las correlaciones entre variables

Primero, para los heatmaps lo óptimo será enseñar en primer lugar un heatmap general de todas las variables, sin variables ni correlaciones ni nada:
"""

correlation_matrix = dfneu.corr()
mask = np.zeros_like(correlation_matrix, dtype=bool)
mask[np.triu_indices_from(mask)] = True
correlation_matrix[mask] = np.nan

whole_corr_matrix = correlation_matrix

plt.figure(figsize=(15, 15))
#sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
ax = sns.heatmap(whole_corr_matrix, annot=False, cmap='coolwarm',xticklabels = False, yticklabels = False)#, fmt=".2f")
plt.title('Matriz de Correlaciones',size = 20)

cbar = ax.collections[0].colorbar
cbar.ax.tick_params(labelsize=20)

plt.show()

"""Tras eso, se pondraçá otras 3 figuras ampliando las tres regiones principales del heatmap: parte superior izquierda, parte inferior derecha y parte inferior izquierda del heatmap:

"""

# Seleccionar la primera submatriz de 36x36
first_corr_matrix = correlation_matrix.iloc[0:36, 0:36]

# Aumentar el tamaño de la figura
plt.figure(figsize=(25, 25))

# Crear el heatmap con un tamaño de fuente ajustado para las anotaciones
ax = sns.heatmap(first_corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 15,"rotation":45},vmin=-1, vmax=1)

plt.xticks(fontsize=20)
plt.yticks(fontsize=20)
#plt.show()

cbar = ax.collections[0].colorbar
cbar.ax.tick_params(labelsize=20)

plt.title('Matriz de Correlaciones - Parte superior izquierda', fontsize=20)
plt.show()

# Seleccionar la primera submatriz de 36x36
second_corr_matrix = correlation_matrix.iloc[37:74, 37:74]

# Aumentar el tamaño de la figura
plt.figure(figsize=(25, 25))

# Crear el heatmap con un tamaño de fuente ajustado para las anotaciones
ax = sns.heatmap(second_corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 15,"rotation":45},vmin=-1, vmax=1)


plt.xticks(fontsize=20)
plt.yticks(fontsize=20)
#plt.show()

cbar = ax.collections[0].colorbar
cbar.ax.tick_params(labelsize=20)

plt.title('Matriz de Correlaciones - Parte inferior derecha', fontsize=20)
plt.show()

# Seleccionar la primera submatriz de 36x36
third_corr_matrix = correlation_matrix.iloc[37:74, 0:36]

# Aumentar el tamaño de la figura
plt.figure(figsize=(25, 25))

# Crear el heatmap con un tamaño de fuente ajustado para las anotaciones
ax = sns.heatmap(third_corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 15,"rotation":45},vmin=-1, vmax=1)

plt.xticks(fontsize=20)
plt.yticks(fontsize=20)
#plt.show()

cbar = ax.collections[0].colorbar
cbar.ax.tick_params(labelsize=20)

plt.title('Matriz de Correlaciones - Parte inferior izquierda', fontsize=20)
plt.show()

"""Se ven los numeros pequeños, pero aún así se aprecia. Como nuestra variable respuesta será neuro, eliminaremos neuro2 y neuro3 que nos aporta la misma info y group.

Entonces, se eliminarán variables que tengan correlación cercana a 1 o -1 porque si dos variables tienen una correlación cercana a 1 o -1, en este caso escogeremos que sean mayor o igual a 0.7 o -0.7, significa que están altamente correlacionadas. En este caso, se eliminará una de las variables para evitar multicolinealidad, que puede afectar la interpretación de los modelos estadísticos.
Y tambien se eliminarán variables redundantes, que proporcionan información similar o redundante, se eliminará una de ellas para simplificar el modelo sin perder mucha información.

Por estas razones, se decidem eliminar las siguientes variables: rhtm2 que nos proporciona la mismo info que rhtm. De igual manera se eliminará nuerStk2 y nuerStk3. Y rsten2, lsten2Y hext que mantiene correlación directa con la mayoría de variables.

Vemos que lplacca y Istenosis tienen corr de 0.86, eliminaremos lplacca ya que tiene mas corr altas con otras variables, igual que rplacca.
Igual con anastcorg y nanast, eliminaremos nanast
nuerStk y Stroke tienen corr 1, eliminaremos nuerStk
Y surgtype que nos da la misma info que surgType.

Eliminamos anastacor que nos propornciona info similar a anastacorg, tambien offpn2 y nos quedamos con offpum. De igual modo, con Bilat y bil2, eliminamos bil2. Y con timecpb y timeac, eliminamos timecpb.

Por los mismos motivos eliminaremos occlus, lstenosis, llocation, lnumber, lside y erythr.

También se eliminará idstata ya que es una variable categorica que identifica a cada paciente y a la hora de realizar modelos no es necesaria

"""

#Eliminamos las variables
dfneu = dfneu.drop(['idstata', 'death', 'neuro2','group','neuro3','rhtm2','nuerStk2','nuerStk3','hext','rsten2', 'lsten2', 'lplacca','rplacca','nanast','nuerStk','surgtype',
                    'anastcor','offpn2','bil2','timecpb', 'occlus','lstenosis','rstenosis', 'llocation','lnumber','lside', 'erythr'], axis=1)

import seaborn as sns
correlation_matrix = dfneu.corr()
plt.figure(figsize=(40, 40))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlación')
plt.show()

dfneu.describe()

"""Nos quedamos con un total de 48 columas

## ESTADISTICOS

### MEDIANA
"""

# Calcular la mediana para variables numéricas
numeric_vars = dfneu.select_dtypes(include=['number'])
median_numeric = numeric_vars.median()

# Calcular el valor máximo para variables numéricas
max_numeric = numeric_vars.max()

# Calcular el valor mínimo para variables numéricas
min_numeric = numeric_vars.min()

# Calcular la desviación estándar para variables numéricas
std_numeric = numeric_vars.std()

# Calcular el rango para variables numéricas
range_numeric = numeric_vars.max() - numeric_vars.min()

# Configurar pandas para mostrar todas las filas
pd.set_option('display.max_rows', None)

# Mostrar los resultados
print("Mediana para variables numéricas:")
print(median_numeric)

print("Std para variables numéricas:")
print(std_numeric)

"""### RANGO"""

print("\nRango para variables numéricas:")
print(range_numeric)

print("\nMáximo para variables numéricas:")
print(max_numeric)

print("\nMínimo para variables numéricas:")
print(min_numeric)

"""## HISTOGRAMAS"""

# Seleccionar las columnas que son numéricas
numeric_vars = dfneu.select_dtypes(include=['number'])

# Mostrar las variables numéricas
print("Variables numéricas:")
print(numeric_vars.columns)

# Seleccionar las columnas que son numéricas
numeric_vars = dfneu.select_dtypes(include=['number'])

# Configurar el layout de los gráficos
num_vars = len(numeric_vars.columns)
nrows = (num_vars // 3) + (1 if num_vars % 3 != 0 else 0)
fig, axes = plt.subplots(nrows=nrows, ncols=7, figsize=(12, 2*nrows))  # Reducir el tamaño de la figura total
axes = axes.flatten()

# Generar los histogramas
for i, name in enumerate(numeric_vars.columns):
    axes[i].hist(dfneu[name], bins=30, alpha=0.75, edgecolor='black')
    axes[i].set_title(name, fontsize=12)  # Reducir el tamaño de la fuente del título
    axes[i].tick_params(axis='both', which='major', labelsize=8)  # Reducir el tamaño de la fuente de las etiquetas de los ejes

# Eliminar subplots vacíos si hay menos gráficos que subplots creados
for j in range(num_vars, len(axes)):
    fig.delaxes(axes[j])

# Ajustar el layout para evitar superposición
plt.tight_layout()

# Mostrar los gráficos
plt.show()

# Seleccionar las columnas que son numéricas
numeric_vars = dfneu.select_dtypes(include=['number'])

# Configurar el layout de los gráficos
num_vars = len(numeric_vars.columns)
nrows = 4
fig, axes = plt.subplots(nrows=nrows, ncols=4, figsize=(9, 10))  # Reducir el tamaño de la figura total
axes = axes.flatten()

# Generar los histogramas
for i, name in enumerate(numeric_vars.columns[0:16]):
    axes[i].hist(dfneu[name], bins=20, alpha=0.75, edgecolor='black')
    axes[i].set_title(name, fontsize=12)  # Reducir el tamaño de la fuente del título
    axes[i].tick_params(axis='both', which='major', labelsize=8)  # Reducir el tamaño de la fuente de las etiquetas de los ejes

# Eliminar subplots vacíos si hay menos gráficos que subplots creados
for j in range(num_vars, len(axes)):
    fig.delaxes(axes[j])

# Ajustar el layout para evitar superposición
plt.tight_layout()

# Mostrar los gráficos
plt.show()

# Seleccionar las columnas que son numéricas
numeric_vars = dfneu.select_dtypes(include=['number'])

# Configurar el layout de los gráficos
num_vars = len(numeric_vars.columns)
nrows = 4
fig, axes = plt.subplots(nrows=nrows, ncols=4, figsize=(9, 10))  # Reducir el tamaño de la figura total
axes = axes.flatten()

# Generar los histogramas
for i, name in enumerate(numeric_vars.columns[16:32]):
    axes[i].hist(dfneu[name], bins=20, alpha=0.75, edgecolor='black')
    axes[i].set_title(name, fontsize=12)  # Reducir el tamaño de la fuente del título
    axes[i].tick_params(axis='both', which='major', labelsize=8)  # Reducir el tamaño de la fuente de las etiquetas de los ejes

# Eliminar subplots vacíos si hay menos gráficos que subplots creados
for j in range(num_vars, len(axes)):
    fig.delaxes(axes[j])

# Ajustar el layout para evitar superposición
plt.tight_layout()

# Mostrar los gráficos
plt.show()

# Seleccionar las columnas que son numéricas
numeric_vars = dfneu.select_dtypes(include=['number'])

# Configurar el layout de los gráficos
num_vars = len(numeric_vars.columns)
nrows = 5
fig, axes = plt.subplots(nrows=nrows, ncols=3, figsize=(9, 10))  # Reducir el tamaño de la figura total
axes = axes.flatten()

# Generar los histogramas
for i, name in enumerate(numeric_vars.columns[32:48]):
    axes[i].hist(dfneu[name], bins=20, alpha=0.75, edgecolor='black')
    axes[i].set_title(name, fontsize=12)  # Reducir el tamaño de la fuente del título
    axes[i].tick_params(axis='both', which='major', labelsize=8)  # Reducir el tamaño de la fuente de las etiquetas de los ejes

# Eliminar subplots vacíos si hay menos gráficos que subplots creados
for j in range(num_vars, len(axes)):
    fig.delaxes(axes[j])

# Ajustar el layout para evitar superposición
plt.tight_layout()

# Mostrar los gráficos
plt.show()

"""En definitiva, vemos una clara asimetría o sesgo en algunas de las variables. También vemos cierta diferencia en las unidades de medida de cada variable. Por tanto, seguramente sea necesario transformarlas. Por otro lado, observamos que algunas parecen tener un valor constante.

Por último, vemos si hay valores faltantes.
"""

# Calcular el número de valores faltantes por columna
missing_values_count = dfneu.isnull().sum()

# Calcular el porcentaje de valores faltantes por columna
missing_values_percent = 100 * missing_values_count / len(dfneu)

# Crear un dataframe con el número y el porcentaje de valores faltantes
missing_values_dfneu = pd.DataFrame({'count': missing_values_count, 'percent': missing_values_percent})

# Mostrar el dataframe ordenado por porcentaje de forma descendente
missing_values_dfneu.sort_values('percent', ascending=False)

"""Hay bastantes valores faltantes en el dataset, y están repartidos en diversas variables. En los casos en los que los missing values representan una parte muy pequeña del número de datos de la variable en total, una forma de solucionar este asunto para poder empezar con nuestro análisis podría ser sustituir estos valores faltantes por la media de la variable en cuestión. No obstante, si hay muchos missing values, es posible que lo más sensato sea eliminar directamente esas variables. Por tanto, aquellas varibles con un alto % de missing values (se establecerá como alto más de un 80%) procederán a ser eliminadas.

# PREPROCESO
Previo al preproceso como tal, es necesario tener en cuenta el concepto de data leakeage. Este fenómeno puede ocurrir cuando hacemos un preproceso completo para todos los datos, antes de dividirlos: al final, puede haber procedimientos que den pie a errores. Tomar esta decisión puede llevar a cambiar los datos de entrenamiento teniendo ya en cuenta los datos de prueba, lo que causa precisamente eso que llamamos una fuga de datos, o data leakeage.  

Por tanto, decidimos hacer nuestro preproceso teniendo en cuenta este problema.

## Preproceso sin data leakeage

Este preproceso se lo aplicamos a todos los datos, ya que no hay riesgo de info-leakeage.

Identificamos qué formato tiene cada variable para saber a cuáles le aplicamos ciertos procedimientos de la pipa de preprocesamiento.

Cabe resaltar que podemos realizar parte del preproceso sin info-leakeage, pues tenemos variables categóricas de las que podemos crear dummies, y también podemos quitar columnas constantes y variables con más de 50% de valores faltantes. Sin embargo, hemos decidido realizar estos procesos -excepto el último- mediante la pipeline, en la que vamos a ver la mejor forma de preprocesar con riesgo de information leakage.

Primero identificamos qué tipo de variables tenemos con una función que hemos creado previamente, en cc guardamos las categóricas y en nm las numéricas, esto nos servirá para ver qué preproceso le aplicamos a cada tipo en la pipa.

Por lo que para definir que se van a borrar aquellas columnas con más del 50% de valores faltantes, en el código, creamos el comando Borrar_NAs, el cual aplicamos tanto a variables categóricas (nm) como numéricas (cc).
"""

# Definir función para eliminar columnas con valores faltantes
def drop_missing_columns(df, nm, cc, threshold=0.5):
    missing_percentage = df.isnull().mean()
    missing_columns = missing_percentage[missing_percentage > threshold].index
    df = df.drop(missing_columns, axis=1)

    nm = [col for col in nm if col not in missing_columns]
    cc = [col for col in cc if col not in missing_columns]

    return df, nm, cc


# Definir las funciones para obtener las columnas numéricas y no numéricas
def tipos(df, tipo):
    return [col for col in df.columns if df[col].dtype == tipo]

def no_tipos(df, tipo):
    return [col for col in df.columns if df[col].dtype != tipo]

dfneu2 = dfneu.drop('neuro', axis=1)
y = dfneu['neuro']

nm = tipos(dfneu2, 'float64')
cc = no_tipos(dfneu2, 'float64')

# Verificar si 'neuro' está presente en cc antes de intentar eliminarlo
if 'neuro' in cc:
    cc.remove('neuro')
if 'idstata' in cc:
    cc.remove('idstata')

# Aplicar la función drop_missing_columns
dfneu2, nm, cc = drop_missing_columns(dfneu2, nm, cc)

# Imputar valores faltantes en las columnas numéricas
numeric_imputer = SimpleImputer(strategy='mean')
dfneu2[nm] = numeric_imputer.fit_transform(dfneu2[nm])

# Imputar valores faltantes en las columnas no numéricas
categorical_imputer = SimpleImputer(strategy='most_frequent')
dfneu2[cc] = categorical_imputer.fit_transform(dfneu2[cc])

# Verificar los valores faltantes después de la imputación
print(dfneu2.isnull().sum())

dfneu2

"""Vemos con cuántas variables nos quedamos:"""

dfneu2.shape

"""## Preproceso con data leakeage

En esta parte del preproceso sí hay que tener cuidado con el tema del data-leakeage, por lo que se hace diferenciando datos de entrenamiento y de prueba.

Hacemos una pipa de preproceso numérica, y otra categórica. Más adelante, el algoritmo nos dirá los mejores hiperparámetros.

El primer paso sería hacer una división en datos de entrenamiento y de prueba, siendo estos últimos 1/4 del total.

Al igual que en el apartado anterior, es importante hacer los procedimientos necesarios prestando atención a si las variables afectadas son numéricas o categóricas. Se creará un pipeline para el preproceso numérico, y otro categórico. Explicaremos primero la numérica.
"""

X_train, X_test, y_train, y_test = train_test_split(dfneu2, y, test_size=0.2, random_state=42) # dividimos tanto la X como la y en train y test
print("Imágenes de entrenamiento", X_train.shape)
print("Imágenes de test", X_test.shape)

"""El parámetro random_state en la función train_test_split de scikit-learn se utiliza para controlar la aleatorización aplicada durante la división de los datos en conjuntos de entrenamiento y prueba. Cuando se establece un valor específico para random_state, se asegura que la división sea reproducible, es decir, que siempre se obtendrá la misma división si se proporciona el mismo valor de random_state.

El valor exacto que se elige para random_state no importa realmente, siempre y cuando sea consistente entre las ejecuciones. Es una convención común utilizar valores como 0, 1 o 42, pero la elección no tiene ninguna implicación especial más allá de la reproducibilidad del resultado. El número 42, en particular, es a menudo utilizado como una referencia humorística en la cultura popular, especialmente debido a su asociación con la respuesta a la pregunta fundamental de la vida, el universo y todo según la novela de Douglas Adams, "The Hitchhiker's Guide to the Galaxy".
"""

# Crear pipeline de preproceso numérica
imputer_learner_reg = LinearRegression()
num_transformers = [
  ("imputer_col", SimpleImputer(strategy = "mean")),
  ("imputer_row", IterativeImputer(estimator = imputer_learner_reg))
]
num_imputer = FeatureUnion(transformer_list = num_transformers)

num_pipeline = Pipeline([
  ("borrar_constantes", VarianceThreshold(0)), # borrar constantes
  ("imputer", num_imputer), # Establecer un imputador para rellenar los valores faltantes
  ("scaler", StandardScaler()) # Normalizar los datos numéricos
])

# Crear dos ramas de métodos de imputación categórica por columnas y por filas
imputer_learner_class = KNeighborsClassifier() # Crear un modelo KNN con parámetros por defecto
cat_transformers = [
  ("imputer_col", SimpleImputer(strategy = "most_frequent")),
  ("imputer_row", IterativeImputer(estimator = imputer_learner_class))
]
cat_imputer = FeatureUnion(transformer_list = cat_transformers)

# Crear pipeline de preproceso categórica
cat_pipeline = Pipeline([
  ("encoder", OneHotEncoder(sparse_output=False,handle_unknown='ignore')), # Codificar los datos categóricos y devolver una matriz
  ("borrar_constantes", VarianceThreshold(0)),
  ("imputer", cat_imputer) # Establecer un imputador para rellenar los valores faltantes
])

# Crear pipeline de imputación numérica
preprocessor = ColumnTransformer([
  ("num", num_pipeline, nm), # Aplicar el pipeline numérico a las columnas numéricas
  ("cat", cat_pipeline, cc) # Aplicar el pipeline categórico a las columnas categóricas
])

# Crear pipeline
regressor=LinearRegression()
pipeline = Pipeline([
('preprocessor', preprocessor),
('regressor', regressor)
])

pipeline

dfneu2[1:10]

"""Se han definido dos pipelines (numeric_transformer y categorical_transformer) para el preprocesamiento de las características numéricas y categóricas, respectivamente.
En el numeric_transformer, se utiliza SimpleImputer para manejar los valores perdidos utilizando la estrategia de reemplazo de la media y luego se aplica StandardScaler para estandarizar las características numéricas.
En el categorical_transformer, se utiliza SimpleImputer para manejar los valores perdidos utilizando la estrategia de reemplazo de la moda y luego se aplica OneHotEncoder para convertir las características categóricas en vectores one-hot.
Se utiliza ColumnTransformer para aplicar las transformaciones definidas por numeric_transformer y categorical_transformer a las columnas numéricas y categóricas, respectivamente.
En resumen, el código implementa el preprocesamiento de los datos de acuerdo con las mejores prácticas, es decir, ajustando cualquier transformación (como la media y la desviación estándar en el caso de StandardScaler) solo en los datos de entrenamiento y luego aplicándola a los datos de prueba o validación sin ajustar nuevamente los parámetros.

## AJUSTAMOS HP

Ahora bien, ¿cuál es la mejor combinación de hiperparámetros de nuestro pipeline de preprocesamiento? Para saberlo, definiremos un param_grid en el que escribiremos nuestros imputadores y escaladores, tanto de la parte numérica como de la categórica, para que el algoritmo decida cuál es el mejor.

En este param_grid tan solo escribimos 3 hiperparámetros de preprocesamiento, que son los dos imputadores numérico y categórico, y el escalador numérico. Antes, en el pipeline de cada tipología, pusimos 6 apartados (3 en el numérico, 3 en el categórico). La razón por la que no ponemos todos es porque hay algunos que son fijos, los correspondientes a la parte de no info-leakeage mencionados anteriormente, por lo que solo nos quedamos con los mencionados.

Además del param_grid, definimos una estrategia de validación cruzada para buscar a esos mejores hiperparámetros. Posteriormente, grid_search y grid_search.fit serán los que hagan y determinen la búsqueda de esos hiperparámetros.

Ahora, se define el modelo deseado y los parámetros que desean buscar utilizando GridSearchCV, junto con la pipeline de preprocesamiento:

### REGRESIÓN LOGÍSTICA
"""

# Crear una instancia del clasificador
regre_log = LogisticRegression()

param_grid_logistic = {
    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],
    'classifier__penalty': ['l2'],
    'classifier__max_iter': [100, 200, 300, 500],
}


# Pipeline completa
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('scaler', StandardScaler()),
                           ('classifier', regre_log)])

# Búsqueda de hiperparámetros con validación cruzada estratificada
stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
grid_search = GridSearchCV(pipeline, param_grid_logistic, cv=stratified_cv, scoring='roc_auc_ovr')
grid_search.fit(X_train, y_train)

# Mejores parámetros y puntajes
print("Mejores hiperparámetros:", grid_search.best_params_)
print("Mejor puntuación:", grid_search.best_score_)

"""e ajustan los hiperparámetros específicos de la regresión logística, como C (que controla la regularización) y penalty (el término de penalización, que puede ser 'l1' para regularización L1 o 'l2' para regularización L2).

### ÁRBOL DE DECISIÓN
"""

# Crear una instancia del clasificador
model_arbol = DecisionTreeClassifier()

param_grid_dt = {
    'classifier__max_depth': [None, 5, 10, 20],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4]
}

# Pipeline completa
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', model_arbol)])

# Búsqueda de hiperparámetros con validación cruzada estratificada
stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
grid_search = GridSearchCV(pipeline, param_grid_dt, cv=stratified_cv, scoring='roc_auc_ovr')
grid_search.fit(X_train, y_train)

# Mejores parámetros y puntajes
print("Mejores hiperparámetros:", grid_search.best_params_)
print("Mejor puntuación:", grid_search.best_score_)

"""1. **Creación del clasificador**: `model_arbol = DecisionTreeClassifier()` crea una instancia del clasificador de árbol de decisión.

2. **Definición del grid de hiperparámetros**: `param_grid_dt` especifica una cuadrícula de hiperparámetros que se probarán durante la búsqueda de hiperparámetros. En este caso, se prueban diferentes valores para `max_depth`, `min_samples_split` y `min_samples_leaf`. Con StratifiedKFold, se realiza una validación cruzada estratificada en lugar de la validación cruzada normal, lo que garantiza que cada fold tenga la misma proporción de clases que el conjunto de datos original.

3. **Creación de la pipeline**: `pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model_arbol)])` define una pipeline que consta de un preprocesador (`preprocessor`) y el clasificador de árbol de decisión (`model_arbol`).

4. **Búsqueda de hiperparámetros con validación cruzada**: `GridSearchCV` realiza una búsqueda exhaustiva de hiperparámetros dentro de la cuadrícula especificada (`param_grid_dt`). Utiliza validación cruzada para evaluar el rendimiento de cada combinación de hiperparámetros.
La función GridSearchCV en la biblioteca scikit-learn de Python es una herramienta que automatiza la búsqueda de hiperparámetros óptimos para un modelo utilizando validación cruzada k-fold. En el proceso, sí, scikit-learn realiza automáticamente la normalización o estandarización apropiada en cada fold según la configuración especificada.

5. **Ajuste del modelo**: `grid_search.fit(X_train, y_train)` ajusta la pipeline al conjunto de datos de entrenamiento, probando todas las combinaciones de hiperparámetros y seleccionando la mejor combinación según la métrica de puntuación especificada,  roc_auc_ovr_weighted: Similar a roc_auc_ovr, pero en lugar de tomar un promedio simple de los AUCs de las clases, aplica un peso a cada AUC según la proporción de muestras en cada clase. Esto puede ser útil cuando hay un desequilibrio significativo entre las clases..

6. **Mejores parámetros y puntuación**: `grid_search.best_params_` devuelve los mejores hiperparámetros encontrados durante la búsqueda y `grid_search.best_score_` devuelve la puntuación obtenida con estos mejores hiperparámetros.

En cuanto a los resultados obtenidos:
* max_depth: Este hiperparámetro controla la profundidad máxima del árbol de decisión. Al establecerlo en None, el árbol se expandirá hasta que todas las hojas sean puras o hasta que contengan un número mínimo de muestras.

* min_samples_leaf: Este hiperparámetro establece el número mínimo de muestras requeridas para estar en un nodo hoja. En este caso, se ha establecido en 1, lo que significa que cada hoja del árbol debe contener al menos una muestra.

* min_samples_split: Este hiperparámetro establece el número mínimo de muestras requeridas para dividir un nodo interno. Aquí se ha establecido en 2, lo que significa que se requerirán al menos dos muestras para que ocurra una división en cualquier nodo.

* Una puntuación de 1.0 indica que tu modelo ha alcanzado una precisión perfecta en el conjunto de datos utilizado para la evaluación. Sin embargo, es importante recordar que esto podría ser un indicador de sobreajuste si no se ha evaluado en datos independientes.

### RANDOM FOREST
"""

# Modelo
model_rf = RandomForestClassifier()

# Parámetros a buscar
param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [None, 5, 10, 20],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4]
}

# Pipeline completa
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', model_rf)])

# Búsqueda de hiperparámetros con validación cruzada estratificada
stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
grid_search_rf = GridSearchCV(pipeline, param_grid, cv=stratified_cv, scoring='roc_auc_ovr')
grid_search_rf.fit(X_train, y_train)

# Mejores parámetros y puntajes
print("Mejores hiperparámetros:", grid_search_rf.best_params_)
print("Mejor puntuación:", grid_search_rf.best_score_)

"""En un Random Forest, max_depth controla la profundidad máxima de los árboles individuales en el bosque. Al establecerlo en None, permite que los árboles crezcan hasta que todas las hojas sean puras o hasta que contengan un número mínimo de muestras, lo que es una estrategia común para evitar el sobreajuste.

Por otro lado, n_estimators controla el número de árboles que se utilizan en el bosque. En este caso, se establece en 50, lo que significa que se utilizan 50 árboles en el bosque.

Obtener una puntuación de 1.0 indica que el modelo ha alcanzado una precisión perfecta en los datos utilizados para la evaluación. Sin embargo, como siempre, es importante verificar si el modelo generaliza bien a datos nuevos y no vistos

### SVM RADIAL
"""

# Modelo
model_svc = SVC()

param_grid_svm_radial = {
    'classifier__C':  [2**num for num in range(-5,6)],
    'classifier__gamma':  [2**num for num in range(-5,6)],

}

# Pipeline completa
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('scaler', StandardScaler()),
                           ('classifier', model_svc)])

# Búsqueda de hiperparámetros con validación cruzada estratificada
stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
grid_search_svm = GridSearchCV(pipeline, param_grid_svm_radial, cv=stratified_cv, scoring='f1') # no puedo usar la misma metrica, me da de resultado NA con roc_auc_ovr
grid_search_svm.fit(X_train, y_train)

# Mejores parámetros y puntajes
print("Mejores hiperparámetros:", grid_search_svm.best_params_)
print("Mejor puntuación:", grid_search_svm.best_score_)

"""En un SVM, C es el parámetro de regularización, que controla el comercio entre la maximización del margen y la minimización de la clasificación errónea. Un valor más alto de C indica una penalización más alta por errores de clasificación en el conjunto de entrenamiento, lo que puede llevar a un modelo más ajustado a los datos de entrenamiento.

Por otro lado, gamma es un parámetro que define cuánta influencia tiene un solo ejemplo de entrenamiento. Un valor bajo de gamma significa que los puntos de datos lejanos tienen un impacto grande en la frontera de decisión, mientras que un valor alto de gamma significa que solo los puntos de datos cercanos tienen un impacto en la frontera de decisión.

Una puntuación de 0.987 indica una alta precisión en los datos de evaluación, lo que sugiere que el modelo SVM con los hiperparámetros dados está funcionando muy bien en el conjunto de datos específico utilizado para la evaluación.

### REDES NEURONALES
"""

# Definir la función para crear el modelo
def build_model(hp):
    model = Sequential()
    model.add(Dense(units=hp.Int('units_input', min_value=32, max_value=512, step=32),
                    input_dim=X_train.shape[1],
                    activation='relu'))
    model.add(Dense(units=hp.Int('units_hidden', min_value=32, max_value=512, step=32),
                    activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    # Optimizador y compilación del modelo
    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),
                  loss='binary_crossentropy', #cambiar
                  metrics=['accuracy'])

    return model

# Convertir los datos a arrays NumPy
X_train_nn = np.array(X_train)
y_train_nn = np.array(y_train)

# Dividir los datos para validación cruzada estratificada
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Lista para almacenar métricas de evaluación
accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []

# Validación cruzada
for train_index, val_index in skf.split(X_train_nn, y_train_nn):
    X_train_nn_fold, X_val_fold = X_train_nn[train_index], X_train_nn[val_index]
    y_train_nn_fold, y_val_fold = y_train_nn[train_index], y_train_nn[val_index]

    # Instanciar el sintonizador para cada fold
    tuner = kt.Hyperband(build_model,
                         objective='val_accuracy',
                         max_epochs=10,
                         factor=3,
                         directory='my_dir',
                         project_name='intro_to_kt')

    # Realizar la búsqueda de hiperparámetros
    tuner.search(X_train_nn_fold, y_train_nn_fold, epochs=50, validation_split=0.2)

    # Obtener el mejor modelo
    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
    best_model = tuner.hypermodel.build(best_hps)

    # Entrenar el mejor modelo
    history = best_model.fit(X_train_nn_fold, y_train_nn_fold, epochs=50, validation_split=0.2,
                             callbacks=[EarlyStopping(patience=5)])

    # Evaluar el modelo en el conjunto de validación
    y_pred_fold = (best_model.predict(X_val_fold) > 0.5).astype("int32")

    # Calcular métricas de evaluación
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred_fold))
    precision_scores.append(precision_score(y_val_fold, y_pred_fold))
    recall_scores.append(recall_score(y_val_fold, y_pred_fold))
    f1_scores.append(f1_score(y_val_fold, y_pred_fold))

# Calcular promedio de métricas de evaluación
avg_accuracy = np.mean(accuracy_scores)
avg_precision = np.mean(precision_scores)
avg_recall = np.mean(recall_scores)
avg_f1 = np.mean(f1_scores)

# Imprimir resultados promedio
print("Promedio de Accuracy:", avg_accuracy)
print("Promedio de Precision:", avg_precision)
print("Promedio de Recall:", avg_recall)
print("Promedio de F1 Score:", avg_f1)

"""### BOOSTING"""

# Modelo base para AdaBoost
base_estimator = DecisionTreeClassifier()

# Modelo AdaBoost
model_ada = AdaBoostClassifier(base_estimator=base_estimator, random_state=42)

# Parámetros a buscar
param_grid = {
    'classifier__base_estimator__max_depth': [1, 5, 10, 20],
    'classifier__n_estimators': [50, 100, 200],
    'classifier__learning_rate': [0.01, 0.1, 1.0]
}

# Pipeline completa
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', model_ada)])

# Búsqueda de hiperparámetros con validación cruzada estratificada
stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
grid_search_ada = GridSearchCV(pipeline, param_grid, cv=stratified_cv, scoring='roc_auc_ovr')
grid_search_ada.fit(X_train, y_train)

# Mejores parámetros y puntajes
print("Mejores hiperparámetros:", grid_search_ada.best_params_)
print("Mejor puntuación:", grid_search_ada.best_score_)

# Extraer los mejores hiperparámetros
mejores_hiperparametros_ada = {
    'base_estimator': DecisionTreeClassifier(max_depth=grid_search_ada.best_params_['classifier__base_estimator__max_depth']),
    'n_estimators': grid_search_ada.best_params_['classifier__n_estimators'],
    'learning_rate': grid_search_ada.best_params_['classifier__learning_rate'],
    'random_state': 42
}

# Crear un nuevo AdaBoost con los mejores hiperparámetros
ada_best = AdaBoostClassifier(**mejores_hiperparametros_ada)

# Entrenar el nuevo modelo de AdaBoost con los datos de entrenamiento
ada_best.fit(X_train, y_train)

"""# SE ENTRENAN LOS MODELOS

## CON AJUSTE DE HIPERPARÁMETROS

### REGRESIÓN LOGÍSTICA
"""

# Crear un nuevo modelo de regresión logística con los mejores hiperparámetros
regre_log = LogisticRegression(C=1, penalty='l2', max_iter = 100)  # Se utilizan los mejores hiperparámetros encontrados

# Entrenar el modelo de regresión logística con los datos de entrenamiento
regre_log.fit(X_train, y_train)

"""### ÁRBOLES DE DECISIÓN

Dado que estamos trabajando con un árbol de decisión (DecisionTreeRegressor), el escalado de características no es necesario, ya que los árboles de decisión no se ven afectados por la escala de las características.
"""

# Definir los mejores hiperparámetros para el árbol de decisión
mejores_hiperparametros_arbol = {
    'max_depth': None,
    'min_samples_leaf': 4,
    'min_samples_split': 10
}

# Crear un nuevo árbol de decisión con los mejores hiperparámetros
arbol = DecisionTreeClassifier(**mejores_hiperparametros_arbol)

# Entrenar el árbol de decisión con los datos de entrenamiento
arbol.fit(X_train, y_train)

"""Se define una Pipeline que incluye el preprocesamiento de datos y el modelo de Árbol de Decisión.
Se define un grid de hiperparámetros que incluye opciones para el preprocesamiento y para los parámetros del modelo de Árbol de Decisión (max_depth, min_samples_split, min_samples_leaf).
Se realiza una búsqueda de hiperparámetros con validación cruzada utilizando GridSearchCV.
Se imprimen los mejores hiperparámetros encontrados y la mejor puntuación de R^2.

### RANDOM FOREST

El preprocesamiento definido en la pipeline se encargará de realizar la normalización/estandarización como se especifica en el pipeline.
"""

mejores_hiperparametros_rf = {
    'n_estimators': 200,
    'max_depth': 20,
    'min_samples_leaf': 2,
    'min_samples_split': 2
}


# Crear un nuevo Random Forest con los mejores hiperparámetros
random_forest = RandomForestClassifier(**mejores_hiperparametros_rf)

# Entrenar el Random Forest con los datos de entrenamiento
random_forest.fit(X_train, y_train)

"""Se define una Pipeline que incluye el preprocesamiento de datos y el modelo de Random Forest.
Se define un grid de hiperparámetros que incluye opciones para el preprocesamiento y para los parámetros del modelo de Random Forest (n_estimators, max_depth, min_samples_split, min_samples_leaf).
Se realiza una búsqueda de hiperparámetros con validación cruzada utilizando GridSearchCV.
Se imprimen los mejores hiperparámetros encontrados y la mejor puntuación de R^2.

### SVM RADIAL
"""

mejores_hiperparametros_svm = {
    'C': 2,
    'gamma': 0.03125
}

# Crear un nuevo SVM radial con los mejores hiperparámetros
svm_radial = SVC(kernel='rbf', **mejores_hiperparametros_svm)

# Entrenar el SVM radial con los datos de entrenamiento
svm_radial.fit(X_train, y_train)

"""Búsqueda de hiperparámetros para una SVM con kernel radial utilizando la métrica de puntuación R^2 y la validación cruzada de series temporales. Los mejores hiperparámetros y la mejor puntuación R^2 se imprimen al final.

Se define una Pipeline que incluye el preprocesamiento de datos y el modelo SVR con kernel radial.
Se define un grid de hiperparámetros que incluye opciones para el preprocesamiento y para los parámetros del modelo SVR (C y gamma).
Se realiza una búsqueda de hiperparámetros con validación cruzada utilizando GridSearchCV.
Se imprimen los mejores hiperparámetros encontrados y la mejor puntuación de R^2.

### TWIN SUPPORT VECTOR MACHINE
"""

# Definición del modelo y parámetros
param_grid_tsvm_radial = {
    'Epsilon1': [0.1],
    'Epsilon2': [0.1],
    'C1': [2**(-5), 2**(-3), 2**(-1), 2**(1), 2**(3), 2**(5)],
    'C2': [2**(-5), 2**(-3), 2**(-1), 2**(1), 2**(3), 2**(5)],
    'kernel_type': [0],  # 1 lineal, 2 polinomial, 3 radial
    'kernel_param': [2**(-5), 2**(-3), 2**(1), 2**(3), 2**(5)],
}

keys, values = zip(*param_grid_tsvm_radial.items())

# Generar todas las combinaciones de valores de parámetros
combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]

# Búsqueda de hiperparámetros con validación cruzada estratificada
stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
best_auc = 0

for params_sel in combinations:
    print(params_sel)
    auc_folds = []
    for train_index, test_index in stratified_cv.split(X_train, y_train):
        X_train_inner = X_train.iloc[train_index]
        X_test_inner = X_train.iloc[test_index]
        y_train_inner = y_train.iloc[train_index]
        y_test_inner = y_train.iloc[test_index]

        # Asegúrate de que los datos están en el formato correcto
        X_train_inner = X_train_inner.to_numpy()
        X_test_inner = X_test_inner.to_numpy()
        y_train_inner = y_train_inner.to_numpy()
        y_test_inner = y_test_inner.to_numpy()

        # Inicializar el modelo con los parámetros seleccionados
        twin_svm = TwinSVMClassifier(**params_sel)

        # Entrenar el modelo
        twin_svm.fit(X_train_inner, y_train_inner)

        # Hacer predicciones
        y_pred_prob = twin_svm.decision_function(X_test_inner)

        # Calcular métricas de evaluación
        auc = roc_auc_score(y_test_inner, y_pred_prob)
        auc_folds.append(auc)

    mean_auc = sum(auc_folds) / len(auc_folds)
    if mean_auc > best_auc:
        best_auc = mean_auc
        best_params = params_sel

# Entrenar el mejor modelo encontrado con todos los datos de entrenamiento
best_model = TwinSVMClassifier(**best_params)
best_model.fit(X_train.to_numpy(), y_train.to_numpy())

# Predicciones en el conjunto de prueba
y_test_pred_prob = best_model.decision_function(X_test.to_numpy())

# Convertir probabilidades a etiquetas de clase usando un umbral (por ejemplo, 0.5)
y_test_pred = (y_test_pred_prob > 0).astype(int)

# Calcular la matriz de confusión para el conjunto de prueba
matriz_confusion = confusion_matrix(y_test, y_test_pred)

# Extraer los valores de la matriz de confusión
VP, FN, FP, VN = matriz_confusion.ravel()

# Calcular el Valor Predictivo Positivo (PV+) y el Valor Predictivo Negativo (PV-)
PV_positivo = VP / (VP + FP)
PV_negativo = VN / (VN + FN)

# Imprimir los resultados
print(f"PV+ (Valor Predictivo Positivo): {PV_positivo:.2f}")
print(f"PV- (Valor Predictivo Negativo): {PV_negativo:.2f}")

# Visualizar la matriz de confusión
import seaborn as sns
plt.figure(figsize=(8, 6))
sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues')
plt.title("Matriz de Confusión - Conjunto de Prueba")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.show()

# Mejores parámetros y puntajes
print("Mejores hiperparámetros:", best_params)
print("Mejor puntuación (ROC AUC):", best_auc)

"""### REDES NEURONALES"""

# Obtener los mejores hiperparámetros encontrados por el sintonizador
mejores_hiperparametros_nn = tuner.get_best_hyperparameters(num_trials=1)[0]

# Construir el modelo de red neuronal con los mejores hiperparámetros
redes_neuro = build_model(mejores_hiperparametros_nn)

# Entrenar el modelo de red neuronal con los datos de entrenamiento
redes_neuro.fit(X_train_nn, y_train_nn, epochs=50, validation_split=0.2,
               callbacks=[EarlyStopping(patience=5)])

print(mejores_hiperparametros_nn)

# Obtener los mejores hiperparámetros encontrados por el sintonizador
mejores_hiperparametros_nn = tuner.get_best_hyperparameters(num_trials=1)[0]

# Imprimir los hiperparámetros seleccionados
print("Mejores hiperparámetros:")
for key, value in mejores_hiperparametros_nn.values.items():
    print(f"{key}: {value}")

"""# PREDICCIONES
Primero hacemos las predicciones para los datos de prueba.
"""

# Realizar predicciones con los modelos entrenados

# Predicciones regre log
regre_log_predicciones = regre_log.predict(X_test)

# Predicciones del árbol de decisión
arbol_predicciones = arbol.predict(X_test)

# Predicciones del bosque aleatorio
rf_predicciones = random_forest.predict(X_test)

# Predicciones del SVM radial
svm_predicciones = svm_radial.predict(X_test)

# Predicciones de las NN
redes_predicciones = redes_neuro.predict(X_test)

# Predicciones del AdaBoost
ada_predicciones = ada_best.predict(X_test)


regre_log_auc = roc_auc_score(y_test, regre_log_predicciones, average='weighted', multi_class='ovr')

# Calcular el AUC ponderado para el árbol de decisión
arbol_auc = roc_auc_score(y_test, arbol_predicciones, average='weighted', multi_class='ovr')

# Calcular el AUC ponderado para el bosque aleatorio
rf_auc = roc_auc_score(y_test, rf_predicciones, average='weighted', multi_class='ovr')

# Calcular el AUC ponderado para la máquina de vectores de soporte (SVM)
svm_auc = roc_auc_score(y_test, svm_predicciones, average='weighted', multi_class='ovr')

# Calcular el AUC ponderado para las NN
redes_auc = roc_auc_score(y_test, redes_predicciones, average='weighted', multi_class='ovr')

# Calcular el AUC ponderado para las Boosting
ada_auc = roc_auc_score(y_test, ada_predicciones, average='weighted', multi_class='ovr')

# Imprimir los resultados
print("Regresión logística AUC (roc_auc_ovr_weighted):", regre_log_auc)
print("Árbol de decisión AUC (roc_auc_ovr_weighted):", arbol_auc)
print("Bosque aleatorio AUC (roc_auc_ovr_weighted):", rf_auc)
print("Máquina de vectores de soporte (SVM) AUC (roc_auc_ovr_weighted):", svm_auc)
print("NN AUC (roc_auc_ovr_weighted):", svm_auc)
print("AdaBoost AUC (roc_auc_ovr_weighted):", ada_auc)

"""ROC AUC (Área bajo la curva ROC) es una métrica comúnmente utilizada para evaluar la calidad de un modelo de clasificación binaria. Mide la capacidad de un modelo para distinguir entre las clases positiva y negativa.

La sigla "OV" en "ROC AUC OVR" significa "One-Versus-Rest" (Uno-contra-Resto). Esto se refiere a la estrategia de extensión de la curva ROC para problemas de clasificación multiclase. En lugar de calcular una sola curva ROC para un problema de clasificación con múltiples clases, el enfoque OVR calcula una curva ROC para cada clase individualmente, tratando esa clase como la clase positiva y todas las demás como la clase negativa. Luego, el ROC AUC OVR promedia los resultados de estas curvas ROC individuales para obtener una única métrica.

Por tanto, los resultados para Regre log, árbol de decisión y rf, sob bastante aceptables, teniendo en cuenta que todos están alrededor de 0.75.

El menos deseable es SVM con 0.5, teniendo en cuenta que trabajamos con datos medicos, no podemos acpetar esa metrica, por lo que nos quedariamos con los anteriores

# MATRIZ DE CONFUSIÓN

## REGRESION LOG
"""

# Calcula la matriz de confusión
matriz_confusion = confusion_matrix(y_test, regre_log_predicciones)

# Extrae los valores de la matriz de confusión
VP, FN, FP, VN = matriz_confusion.ravel()

# Calcula el Valor Predictivo Positivo (PV+) y el Valor Predictivo Negativo (PV-)
PV_positivo = VP / (VP + FP)
PV_negativo = VN / (VN + FN)

# Imprime los resultados
print(f"PV+ (Valor Predictivo Positivo): {PV_positivo:.2f}")
print(f"PV- (Valor Predictivo Negativo): {PV_negativo:.2f}")

"""## ÁRBOL"""

# Calcula la matriz de confusión
matriz_confusion = confusion_matrix(y_test, arbol_predicciones)

# Extrae los valores de la matriz de confusión
VP, FN, FP, VN = matriz_confusion.ravel()

# Calcula el Valor Predictivo Positivo (PV+) y el Valor Predictivo Negativo (PV-)
PV_positivo = VP / (VP + FP)
PV_negativo = VN / (VN + FN)

# Imprime los resultados
print(f"PV+ (Valor Predictivo Positivo): {PV_positivo:.2f}")
print(f"PV- (Valor Predictivo Negativo): {PV_negativo:.2f}")

"""## RF"""

# Calcula la matriz de confusión
matriz_confusion = confusion_matrix(y_test, rf_predicciones)

# Extrae los valores de la matriz de confusión
VP, FN, FP, VN = matriz_confusion.ravel()

# Calcula el Valor Predictivo Positivo (PV+) y el Valor Predictivo Negativo (PV-)
PV_positivo = VP / (VP + FP)
PV_negativo = VN / (VN + FN)

# Imprime los resultados
print(f"PV+ (Valor Predictivo Positivo): {PV_positivo:.2f}")
print(f"PV- (Valor Predictivo Negativo): {PV_negativo:.2f}")

"""## SVM"""

# Calcula la matriz de confusión
matriz_confusion = confusion_matrix(y_test, svm_predicciones)

# Extrae los valores de la matriz de confusión
VP, FN, FP, VN = matriz_confusion.ravel()

# Calcula el Valor Predictivo Positivo (PV+) y el Valor Predictivo Negativo (PV-)
PV_positivo = VP / (VP + FP)
PV_negativo = VN / (VN + FN)

# Imprime los resultados
print(f"PV+ (Valor Predictivo Positivo): {PV_positivo:.2f}")
print(f"PV- (Valor Predictivo Negativo): {PV_negativo:.2f}")

"""## BOOSTING"""

# Calcular la matriz de confusión
matriz_confusion = confusion_matrix(y_test, ada_predicciones)

# Extraer los valores de la matriz de confusión
VP, FN, FP, VN = matriz_confusion.ravel()

# Calcular el Valor Predictivo Positivo (PV+) y el Valor Predictivo Negativo (PV-)
PV_positivo = VP / (VP + FP)
PV_negativo = VN / (VN + FN)

# Imprimir los resultados
print(f"PV+ (Valor Predictivo Positivo): {PV_positivo:.2f}")
print(f"PV- (Valor Predictivo Negativo): {PV_negativo:.2f}")

"""# GRAFICOS

## REGRESION LOG
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score

# Obtener las probabilidades de predicción para las clases positivas
y_prob_log = regre_log.predict(X_test)

# Calcula la curva ROC
fpr_log, tpr_log, thresholds_log = roc_curve(y_test, y_prob_log)

# Calcula el AUC
roc_auc_log = auc(fpr_log, tpr_log)

# Crea el gráfico
plt.figure()
plt.plot(fpr_log, tpr_log, color='purple', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_log)
plt.plot([0, 1], [0, 1], color='lightgreen', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Regresión Logística')
plt.legend(loc="lower right")
plt.show()

"""Este código calculará la curva ROC y el AUC utilizando las probabilidades de predicción del modelo de regresión logística entrenado en el conjunto de datos de prueba. Luego, trazará la curva ROC con el área bajo la curva (AUC) en el gráfico. Asegúrate de tener importadas las bibliotecas necesarias (matplotlib.pyplot y roc_curve, auc de sklearn.metrics).

## ÁRBOL

### Grafico arbol
"""

feature_names = X_test.columns.tolist()

plt.figure(figsize=(15, 8))
plot_tree(arbol, filled=True, feature_names=feature_names)
plt.show()

"""### Grafico ROC"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score

# Obtener las probabilidades de predicción para las clases positivas
y_prob = arbol.predict(X_test)

# Calcula la curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_prob)

# Calcula el AUC
roc_auc = auc(fpr, tpr)

# Crea el gráfico
plt.figure()
plt.plot(fpr, tpr, color='purple', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='lightgreen', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Árbol de decisión')
plt.legend(loc="lower right")
plt.show()

"""Este código utilizará las probabilidades de predicción del modelo de árbol de decisión entrenado en el conjunto de datos de prueba para calcular la curva ROC y el AUC. Luego, trazará la curva ROC con el área bajo la curva (AUC) en el gráfico. Asegúrate de tener importadas las bibliotecas necesarias (matplotlib.pyplot y roc_curve, auc de sklearn.metrics).

## RF

### Grafico RF
"""

# Extraer un árbol del bosque aleatorio (por ejemplo, el primero)
arbol_individual = random_forest.estimators_[0]

plt.figure(figsize=(15, 8))
plot_tree(arbol_individual, filled=True, feature_names=feature_names)
plt.show()

"""### Grafico ROC"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score

# Configurar el tamaño de fuente global
plt.rcParams.update({'font.size': 8})

# Obtener las probabilidades de predicción para las clases positivas
y_prob_rf = random_forest.predict(X_test)

# Calcula la curva ROC
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_prob_rf)

# Calcula el AUC
roc_auc_rf = auc(fpr_rf, tpr_rf)

# Crea el gráfico
plt.figure()
plt.plot(fpr_rf, tpr_rf, color='purple', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_rf)
plt.plot([0, 1], [0, 1], color='lightgreen', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Random Forest')
plt.legend(loc="lower right")
plt.show()

"""Este código calculará la curva ROC y el AUC utilizando las probabilidades de predicción del modelo de Random Forest entrenado en el conjunto de datos de prueba. Luego, trazará la curva ROC con el área bajo la curva (AUC) en el gráfico. Asegúrate de tener importadas las bibliotecas necesarias (matplotlib.pyplot y roc_curve, auc de sklearn.metrics).

## SVM
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score

# Obtener las probabilidades de predicción del modelo SVM radial para el conjunto de datos de prueba
y_score = svm_radial.predict(X_test)

# Calcular la curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)

# Graficar la curva ROC
plt.figure()
plt.plot(fpr, tpr, color='purple', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='lightgreen', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Receiver Operating Characteristic - SVM Radial')
plt.legend(loc="lower right")
plt.show()

"""## NN"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score

# Obtener las probabilidades de predicción del modelo de red neuronal para el conjunto de datos de prueba
y_pred_proba = redes_neuro.predict(X_test)

# Calcular la curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Calcular el área bajo la curva ROC (AUC)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='purple', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='lightgreen', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Redes Neuronales')
plt.legend(loc="lower right")
plt.show()

"""Este código utilizará las probabilidades de predicción del modelo de red neuronal para calcular la curva ROC y el área bajo la curva (AUC). Luego, trazará la curva ROC con el área bajo la curva en el gráfico. Asegúrate de tener importadas las bibliotecas necesarias (matplotlib.pyplot y roc_curve, auc de sklearn.metrics).

## TSVM
"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Calcula la curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)
roc_auc_value = auc(fpr, tpr)

# Graficar la curva ROC
plt.figure()
plt.plot(fpr, tpr, color='purple', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_value)
plt.plot([0, 1], [0, 1], color='lightgreen', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Receiver Operating Characteristic - TSVM')
plt.legend(loc="lower right")
plt.show()

"""En este caso, clf.decision_function(xtest_np) devuelve las puntuaciones de decisión (o distancias a los hiperplanos) para las muestras de xtest_np, y luego estas puntuaciones se utilizan para calcular la curva ROC y el área bajo la curva (AUC). Asegúrate de tener importada la biblioteca matplotlib.pyplot y la función roc_curve y auc de sklearn.metrics.

## BOOSTING
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score

# Obtener las probabilidades de predicción del clasificador de boosting para el conjunto de datos de prueba
y_score = ada_best.predict(X_test)

# Calcular la curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)

# Graficar la curva ROC
plt.figure()
plt.plot(fpr, tpr, color='purple', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='lightgreen', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Receiver Operating Characteristic - AdaBoost')
plt.legend(loc="lower right")
plt.show()

"""# ANÁLISIS DE INTERPRETABILIDAD

## LIME
"""

# Entrenar el modelo RandomForest
random_forest = RandomForestClassifier(random_state=0)
random_forest.fit(X_train, y_train)

# Crear el objeto LimeTabularExplainer
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,
                                                   feature_names=X_train.columns.values.tolist(),
                                                   class_names=['No Complicaciones', 'Complicaciones'],
                                                   verbose=True,
                                                   mode='classification')

# Explicar las predicciones para cuatro instancias diferentes
indices = [10, 20, 30, 40]
for i in indices:
    exp = explainer.explain_instance(X_test.values[i], random_forest.predict_proba, num_features=5)
    print(f"Explicación para la instancia {i} (verdadera clase: {y_test.iloc[i]})")
    exp.show_in_notebook(show_table=True)

"""## SHAP"""

# Sandra:

# Creamos un explicador con train
explainer = shap.KernelExplainer(random_forest.predict, X_train)
#explainer_2 = shap.PermutationExplainer(random_forest.predict, X_train)

# Explicamos en test
shap_values = explainer.shap_values(X_test)
#shap_values_2 = explainer_2.shap_values(X_test)

# Ver si dimensiones SHAP y datos coindicen
print("observaciones ",len(shap_values))
print("variables" ,len(shap_values[0]))
print("observaciones ",X_test.shape[0])
print("variables ",X_test.shape[1])

# Iniciamos JS (javascript)
shap.initjs()

print(explainer.expected_value)
print("variables ", len(shap_values[10,:]))
print(shap_values[10,:])
print("variables ",len(X_test.iloc[10, :]))
# Mostramos la explicación para X_test numero 10
shap.force_plot(explainer.expected_value, shap_values[10,:], X_test.iloc[10, :])

def func(x):
    return random_forest.predict_proba(x)[:, 1]

med = X_train.median().values.reshape((1, X_train.shape[1]))

print(med)

explainer_2 = shap.Explainer(func, med)
shap_values_2 = explainer(X_test)

shap_values_2

shap.initjs()

feature_names = []
for col in X_train.columns:
    feature_names.append(col)

expl = Explanation(shap_values_2.values[:,1], shap_values_2.base_values, X_test, feature_names=feature_names)

shap.plots._waterfall.waterfall_legacy(expl.base_values[0], shap_values_2.values[10], feature_names = feature_names, max_display=20)

# Iniciamos JS (javascript)
shap.initjs()

# Mostramos la explicación para X_test numero 20
shap.force_plot(explainer.expected_value, shap_values[20,:], X_test.iloc[20, :])

# Iniciamos JS (javascript)
shap.initjs()

# Mostramos la explicación para X_test numero 30
shap.force_plot(explainer.expected_value, shap_values[30,:], X_test.iloc[30, :])

# Iniciamos JS (javascript)
shap.initjs()

# Mostramos la explicación para X_test numero 40
shap.force_plot(explainer.expected_value, shap_values[40,:], X_test.iloc[40, :])

shap.initjs()
#print("SHAP values son", len(shap_values))

print(len(shap_values))
shap.summary_plot(shap_values, X_test)

"""### Visualizar las importancias de las características para una sola instancia

#  Importancia de variable que da el RF
"""

feature_names = []
for col in X_train.columns:
    feature_names.append(col)

print(feature_names)

# Crear nombres de las características
#feature_names = [f"feature {i}" for i in range(X_train.shape[1])]

feature_names = []
for col in X_train.columns:
    feature_names.append(col)

#print(feature_names)

# Entrenar el RandomForestClassifier
forest = RandomForestClassifier(random_state=0)
forest.fit(X_train, y_train)

# Calcular importancias de las características
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)
forest_importances = pd.Series(importances, index=feature_names)
std = [x for _, x in sorted(zip(forest_importances, std),reverse=True)]
forest_importances = forest_importances.sort_values(ascending = False)
forest_importances= forest_importances.nlargest(15)



# Crear el gráfico de barras
fig, ax = plt.subplots(figsize=(10, 6))  # Ajustar el tamaño de la figura
forest_importances.plot.bar(yerr=std[:15], ax=ax)
ax.set_title("Feature importances using MDI", fontsize=12)  # Ajustar el tamaño de la fuente del título
ax.set_ylabel("Mean decrease in impurity", fontsize=12)  # Ajustar el tamaño de la fuente de la etiqueta del eje y
ax.set_xlabel("Features", fontsize=12)  # Ajustar el tamaño de la fuente de la etiqueta del eje x
ax.tick_params(axis='x', labelsize=10, rotation=45)  # Ajustar el tamaño de las etiquetas del eje x y la rotación
ax.tick_params(axis='y', labelsize=10)  # Ajustar el tamaño de las etiquetas del eje y
fig.tight_layout()

# Mostrar el gráfico
plt.show()